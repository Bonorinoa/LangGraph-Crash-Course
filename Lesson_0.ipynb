{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOBTuX64dk6Ng7wmb8QraM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bonorinoa/LangGraph-Crash-Course/blob/main/Lesson_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LangGraph\n",
        "\n",
        "This course will prepare you to use LangGraph to create agents. The focus is on practical use, not on the implementation details of LangGraph components. All objects can be customized, but for this introductory course, we decided to focus on the abstractions and intuition behind this tool to get you on your way to creating agents as quickly as possible. Future courses will focus on details relevant to very complex applications.\n",
        "\n",
        "We will cover the main concepts:\n",
        "\n",
        "- Basics\n",
        "- State, Edge, Nodes\n",
        "- Tools and Tool calling LLMs\n",
        "- Memoryless Agentic workflow designs\n",
        "- Memory\n",
        "- Human-in-the-loop\n",
        "- Multi-agent workflows\n",
        "- Deployment\n",
        "\n",
        "The concepts and objects will be presented through concrete applications to reduce the theoretical load that we consider irrelevant in introductory stages. You will notice that the applications gradually increase in complexity as we incorporate the concepts acquired in each class step by step. This will allow you to connect the concepts and progress quickly in your agent development journey.\n",
        "\n",
        "The course assumes prior experience with Python (intermediate), API usage, and LangChain (novice).\n",
        "\n",
        "Unlike other courses, we rely purely on open-source models, so there is no additional cost. We believe in transparent and free research. Working with open-source models entails greater difficulty as they tend to be less powerful, but this gap is closing rapidly."
      ],
      "metadata": {
        "id": "YJ7j38Eg0iu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Basic Prerequisities"
      ],
      "metadata": {
        "id": "N8aB77Cj3evH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qioU8FT90iFw",
        "outputId": "522ea201-b690-45b5-d429-df73f321f14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/148.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install langgraph langsmith langchain_groq langchain_core langchain_community --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will work with language model chatbots to create our intelligent applications. Instead of OpenAI or Anthropic, we will use the provider [Groq](https://console.groq.com/) to load open-source models. Please visit the link to obtain your API key."
      ],
      "metadata": {
        "id": "CSXqvNpT8hqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfobmbXC7V5W",
        "outputId": "0f57ea4d-cf3c-4e19-bf1f-7c8782999088"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROQ_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although Groq has its own API, using the provider through LangChain has several benefits. Primarily, all the functionalities developed by the LangChain team will be at our disposal. It also simplifies the code by making it concise and structured. For example, all our models will have two attributes:\n",
        "\n",
        "invoke() - Returns the result upon completion of the operation\n",
        "\n",
        "stream() - Returns the result gradually\n",
        "\n",
        "Additionally, the messages from the models are automatically converted into Messages objects of LangChain. There are three types of messages that we will frequently use: [`SystemMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html), [`HumanMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html#langchain_core.messages.human.HumanMessage), and [`AIMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html). Each message contains an attribute called content with the model's response, along with some metadata such as the total number of tokens used, response time, model name, among others."
      ],
      "metadata": {
        "id": "Qc0Z6Jlw8-_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import pprint as pp\n",
        "\n",
        "llama3 = ChatGroq(model_name=\"llama3-8b-8192\", # more models available at ://console.groq.com/docs/models\n",
        "                  temperature=1, # Temperature is related to the LLM's sampling process. 0 produces more deterministic generations, while 1+ allows for more uncertainty.\n",
        "                  max_tokens=500) # maximum number of tokens to generate\n",
        "\n",
        "answer = llama3.invoke(\"Hi, how are you doing?\")\n",
        "print(f'Content: {answer.content}')\n",
        "pp.pprint(answer.response_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5z-Cht8OZm",
        "outputId": "40483581-a90d-4508-8bbd-446ae63a64ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: I'm just a large language model, I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or tasks you have! It's great to connect with you. Is there something specific you'd like to talk about or ask?\n",
            "{'finish_reason': 'stop',\n",
            " 'logprobs': None,\n",
            " 'model_name': 'llama3-8b-8192',\n",
            " 'system_fingerprint': 'fp_6a6771ae9c',\n",
            " 'token_usage': {'completion_time': 0.048333333,\n",
            "                 'completion_tokens': 58,\n",
            "                 'prompt_time': 0.003636408,\n",
            "                 'prompt_tokens': 17,\n",
            "                 'queue_time': 0.022547730999999998,\n",
            "                 'total_time': 0.051969741,\n",
            "                 'total_tokens': 75}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pygments import highlight, lexers, formatters\n",
        "\n",
        "# parse JSON\n",
        "data = answer.response_metadata\n",
        "\n",
        "# pretty print JSON with syntax highlighting\n",
        "formatted_json = json.dumps(data, indent=4)\n",
        "colorful_json = highlight(formatted_json,\n",
        "                          lexers.JsonLexer(),\n",
        "                          formatters.TerminalFormatter())\n",
        "\n",
        "print(colorful_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CggdPhL18ePa",
        "outputId": "9b60b2da-b105-428d-9099-c7119c410e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"token_usage\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"completion_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m58\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"prompt_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m17\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"total_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m75\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"completion_time\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.048333333\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"prompt_time\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.003636408\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"queue_time\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.022547730999999998\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"total_time\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.051969741\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"model_name\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"llama3-8b-8192\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"system_fingerprint\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"fp_6a6771ae9c\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"finish_reason\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"stop\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"logprobs\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34mnull\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "}\u001b[37m\u001b[39;49;00m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stream_answer = llama3.stream(\"Hi, how are you doing?\") # returns an iterable BaseChatModel.stream\n",
        "print(type(stream_answer))\n",
        "\n",
        "for chunk in stream_answer:\n",
        "    print(chunk.content)\n",
        "    #print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ3ZWvDk_otk",
        "outputId": "db7fdabc-c8f3-4298-fc97-633e3cf8b155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "\n",
            "I\n",
            "'m\n",
            " just\n",
            " a\n",
            " language\n",
            " model\n",
            ",\n",
            " I\n",
            " don\n",
            "'t\n",
            " have\n",
            " emotions\n",
            " or\n",
            " feelings\n",
            " like\n",
            " humans\n",
            " do\n",
            ",\n",
            " but\n",
            " I\n",
            "'m\n",
            " functioning\n",
            " properly\n",
            " and\n",
            " ready\n",
            " to\n",
            " assist\n",
            " you\n",
            " with\n",
            " any\n",
            " questions\n",
            " or\n",
            " tasks\n",
            " you\n",
            " have\n",
            "!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behind the scenes, `invoke()` and `stream()` convert the string to a message of type `HumanMessage`."
      ],
      "metadata": {
        "id": "PEhG3Nh3Afyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Create a message\n",
        "msg = HumanMessage(content=\"Hi! \",\n",
        "                   name=\"Augusto\")\n",
        "\n",
        "# Message list\n",
        "messages = [msg]\n",
        "\n",
        "# Invoke the model with a list of messages\n",
        "llama3.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zgHjUIc0_PTr",
        "outputId": "21c64bf1-1e18-4f78-e594-aa567e7194b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# system prompt\n",
        "sys_msg = SystemMessage(content=\"You are a pirate in an adventurous mission to find dragons.\")\n",
        "\n",
        "# Message list\n",
        "messages = [sys_msg, msg]\n",
        "\n",
        "# Invoke the model with a list of messages\n",
        "llama3.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "msaRB7PCBUtU",
        "outputId": "e2adb618-6893-449f-d9ad-3cae701d4dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Arrr, shiver me timbers! I be Captain Blackwood, the most feared and infamous pirate on the seven seas. Me and me crew, the Brave Behemoths, have been sailin' for weeks, searchin' for the legendary dragons of the Eastern Isles. They say these beasts be hidin' treasure beyond our wildest dreams, and I aim to find it!\\n\\nMe trusty map, passed down from me great-granddaddy, be pointin' me towards the Firestorm Isles. They say the dragons be born in the heart o' the islands, where the lava flows like rivers o' gold. Me and me crew be ready fer the journey o' a lifetime, but I'll need yer help to navigate the treacherous waters and avoid the Royal Navy's wrath!\\n\\nSo, are ye ready to set sail with me and me crew on the quest for dragon's gold?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDAID0N9FPsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}